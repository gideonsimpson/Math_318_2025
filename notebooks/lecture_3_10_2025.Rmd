---
title: "Lecture 3/10/2025"
output: html_document
date: "2025-03-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggfortify) # this provides PCA, k-Means
library(mvtnorm)
```

# Motivation for Unsupervised Methods

```{r}
set.seed(100)
mu = c(3,-5)
sigma = matrix(c(2, 1, 1, 2), nrow=2)
xy.samples = as_tibble(rmvnorm(10^3, mean = mu, sigma = sigma))
colnames(xy.samples) = c("x","y")
xy.plt = ggplot(data=xy.samples, mapping = aes(x=x,y=y)) + 
  geom_point()
print(xy.plt)
```

What are the dominant directions in this data set?

# PCA

## Toy Problem

Compute the two principal components

```{r}
pr.out = prcomp(xy.samples)
pr.out
```

Squares of the standard devs are the eigenvalues:

```{r}
pr.out$sdev^2
```

These are $\lambda_1$ and $\lambda_2$.

Proportion of variance explained (PVE):

```{r}
pr.var=pr.out$sdev^2
pve = pr.var/sum(pr.var)
pve
```

About 75% of the variance is in the PC1 direction, and 25% of the variance is in PC2.

**NOTE**: We did not need to center our data; `prcomp` does that for us. We also did not scale the data.

Suppose we scale our data (in general, we should):

```{r}
pr.scaled.out = prcomp(xy.samples, scale=TRUE)
pr.scaled.out
```

```{r}
pr.scaled.var = pr.scaled.out$sdev^2
pve.scaled = pr.scaled.var/sum(pr.scaled.var)
pve.scaled
```

```{r}
names(pr.scaled.out)
```

```{r}
pr.scaled.out$center
```

## Biplot visualization

See how data is distributed along the PC1-PC2 axes:

```{r}
pca.scaled.plt = autoplot(pr.scaled.out) + 
  ggtitle("PCA Scaled/No Loadings")
print(pca.scaled.plt)
```

```{r}

pca.scaled.loadings.plt = autoplot(pr.scaled.out,loadings=TRUE,
                                    loadings.label=TRUE) + 
  ggtitle("PCA Scaled/With Loadings")
print(pca.scaled.loadings.plt)
```

## Iris Data

This runs PCA on <https://archive.ics.uci.edu/dataset/53/iris>, the iris data set. For each flower, separate from the class, we have 4 features.

```{r}
iris.df=read_csv("../data/iris/iris.data",col_names=FALSE, show_col_types = FALSE)
colnames(iris.df) = c("sepal.length", "sepal.width", 
                       "petal.length", "petal.width", "class")
iris.df$class = as_factor(iris.df$class)

# compute principle components
pca.iris = prcomp(select_if(iris.df,is.numeric),scale=TRUE)
pr.var = pca.iris$sdev^2
pve = pr.var/sum(pr.var)

pve
```

Over 95% of the data's variation can be explained with just two PCs. We do not need all 4 features.

```{r}
iris.plt = autoplot(pca.iris,loadings=TRUE,loadings.label=TRUE) +  
  ggtitle("PCA of Iris Data")
print(iris.plt)
```

# Motivation for k-Means

```{r}
set.seed(100)
mu1 = c(4,-6)
sigma1 = matrix(c(2, 1, 1, 2), nrow=2)
mu2 = c(-1,2)
sigma2 = matrix(c(3, -0.1, -0.1, 3), nrow=2)
samples1 = rmvnorm(10^3, mean = mu1, sigma = sigma1)
samples2 = rmvnorm(10^3, mean = mu2, sigma = sigma2)
clusters.samples = as_tibble(rbind(samples1, samples2))
colnames(clusters.samples) = c("x1","x2")
clusters.plt = ggplot(data=clusters.samples, 
                      mapping = aes(x=x1,y=x2)) + 
  geom_point()
print(clusters.plt)
```
