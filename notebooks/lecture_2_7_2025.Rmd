---
title: "Lecture 2/7/2025"
output: html_document
date: "2025-02-05"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(ISLR2)
```

# Classifiers

## Logistic Regression

Look `Default` credit card data set

```{r}
logistic.glm.plt = ggplot(Default, 
                          aes(x=balance, y = as.numeric(default) -1)) + 
  geom_point()

logistic.glm.plt = logistic.glm.plt + 
  geom_smooth(method = glm, formula = y~x, method.args = list(family="binomial"))

print(logistic.glm.plt)
```

We have used `glm` instead of `lm` , and we have used `binomial` to set to logistic regression.

Getting the underlying classifier model:

```{r}
logistic.glm = glm(default~balance, data=Default, family = binomial)
```

```{r}
logistic.glm
```

`(Intercept)` is the $\beta_0$ and `balance` is the $\beta_1$

The coefficients are the $\beta_0$ and the $\beta_1$ in the logistic model.

```{r}
summary(logistic.glm)
```

and we can get confidence intervals on the coefficients

```{r}
confint(logistic.glm)
```

## Predictions and Accuracy

To the predictions, we use `predict` :

```{r}
# defaults to predicting the responses in training data
pred.prob = predict(logistic.glm, type="response") 
pred.prob[1:5]
```

`pred.prob` contains the predicted default probabilities. To turn these into classes (`No` or `Yes` ):

```{r}
default.pred = rep("No", length(pred.prob)) # set everyone to No initially,
default.pred[pred.prob>0.5]="Yes" # flips all the indices where p > 0.5 to Yes
```

Check accuracy:

```{r}
mean(as.numeric(default.pred != Default$default))
```

The above computes:

$$
\frac{1}{n} 1_{\hat{y}_i \neq y_i}
$$

The error rate. 2.75%.

## Confusion Matrix

```{r}
table(default.pred, Default$default)
```

What our model is missing. The off diagonal entries are the mismatches. This model predicts 142 defaults, even though 42 of them will not. The data has 333 defaults, and the model only catches 100 of them.
