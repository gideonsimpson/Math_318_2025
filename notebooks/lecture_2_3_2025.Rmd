---
title: "Lecture 2/3/2025"
output: html_document
date: "2025-02-02"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(ISLR2)
```

# Multilinear Regression

## Example with Auto Dataset

```{r}
auto.df = as_tibble(Auto)
```

```{r}
auto.df
```

```{r}
auto.df$origin = as_factor(auto.df$origin)
auto.df$cylinders= as_factor(auto.df$cylinders)
```

Want to model `mpg` against the other features. `mpg` plays the role of the $Y$, the response variable.

The first step is to check pair correlations to see what actually matters:

```{r}
cor(select_if(auto.df, is.numeric)) # check pair correlations amongst numerical features
```

The three most strongly correlated predictors are `horsepower` , `displacement` , and `weight` .

Fit the model:

```{r}
auto.lm = lm(mpg~displacement+weight+horsepower, auto.df)
```

```{r}
auto.lm
```

```{r}
summary(auto.lm)
```

We get an $R^2\approx .7$. The $\hat{\beta}_1$ for `displacement` is not statistically significant:

```{r}
confint(auto.lm)
```

The others are statistically significant. We now iterate:

-   Drop `displacement` from the model

-   Try `year` instead, as it is the next most strongly correlated

```{r}
auto.lm2 = lm(mpg~weight+horsepower+year, auto.df)
summary(auto.lm2)
```

Remarks:

-   This is a better fit, as the $R^2$ is higher

-   This also suggests we do not need the `horsepower` . Previously, the horsepower was statistically significant.

Lets drop `horsepower` and try the last one:

```{r}
auto.lm3 = lm(mpg~weight+acceleration+year, auto.df)
summary(auto.lm3)
```

Note, we can extract just the $R^2$:

```{r}
auto3.summary = summary(auto.lm3)
```

```{r}
auto3.summary$r.squared
```

Final model:

```{r}
auto.lm4 = lm(mpg~weight+year, auto.df)
summary(auto.lm4)
```

This has the highest $R^2$, with only two predictors. Including more features is not necessarily better.

Check an alternative:

```{r}
auto.lm5 = lm(mpg~weight+acceleration, auto.df)
summary(auto.lm5)
```

Note the order in which we add features matters:

-   (`weight`, `acceleration`, `year` ) is much better than (`weight`, `acceleration`)

-   But (`weight`, `acceleration`, `year` ) is about the same as (`weight`, `year`)

### Collapse down the model

Start with all features, and sequentially remove them:

```{r}
names(auto.df)
```

```{r}
collapse.lm1 = lm(mpg~displacement+horsepower+weight+acceleration+year, auto.df)
summary(collapse.lm1)
```

We can definitely eliminate `horsepower` :

```{r}
collapse.lm2 = lm(mpg~displacement+weight+acceleration+year, auto.df)
summary(collapse.lm2)
```

We can definitely eliminate `displacement`:

```{r}
collapse.lm3 = lm(mpg~+weight+acceleration+year, auto.df)
summary(collapse.lm3)
```

We would next drop `acceleration`

You can drop more than one variable per iteration.

This kind of manual, sequential iteration is easy enough to do with \~10 predictors. We will need something else for \~100 predictors. This leads to LASSO.

# Nonlinear Fitting

## Motivation

```{r}
set.seed(100)
x = runif(100, min=0, max = 4)
y = x + .5 * x**2 -.25 * x**3 + rnorm(100, sd=.1)
df = tibble(x=x, y=y)
```

```{r}
xy.plt = ggplot(df, aes(x,y)) + geom_point()
print(xy.plt)
```

This is a plot of

$$
Y = X + .5 X^2 -.25 X^3 + \epsilon 
$$

We will fit a **linear** model with **nonlinear** features.

## Try a series of polynomial models

#### Linear

This is bad, but this is an example of a bad fit:

```{r}
plt = ggplot(df, aes(x,y)) + geom_point() + 
  geom_smooth(method=lm, formula = y~x)
print(plt)
```

#### Quadratic

```{r}
quad.lm = lm(y~x+I(x^2), df) 
summary(quad.lm)
```

Note that the `I(x^2)` instructs it to compute the nonlinear transform of the predictor and regress with that.

This command solves the normal equations:

$$
\mathcal{X}^T\mathcal{X}\boldsymbol{\beta} = \mathcal{X}^T \boldsymbol{y}
$$

See how well this does, graphically: use `I` inside of `ggplot2`

```{r}
plt = ggplot(df, aes(x,y)) + geom_point() + 
  geom_smooth(method=lm, formula = y~x + I(x^2))
print(plt)
```

#### Cubic

```{r}
plt = ggplot(df, aes(x,y)) + geom_point() + 
  geom_smooth(method=lm, formula = y~x + I(x^2)+I(x^3))
print(plt)
```

```{r}
cubic.lm = lm(y~x+I(x^2)+I(x^3), df) 
summary(cubic.lm)
```

#### Quartic

```{r}
plt = ggplot(df, aes(x,y)) + geom_point() + 
  geom_smooth(method=lm, formula = y~x + I(x^2)+I(x^3)+I(x^4))
print(plt)
```

```{r}
quartic.lm = lm(y~x+I(x^2)+I(x^3)+I(x^4), df) 
summary(quartic.lm)
```

Notice the change in the $p$ values when we increase further, even though the fit is still quite good.

### General case

Rather than having to keep adding `I(x^k)` term, we can use `poly`, with the specified highest degree

```{r}
plt = ggplot(df, aes(x,y)) + geom_point() + 
  geom_smooth(method=lm, formula = y~poly(x,3))
print(plt)
```

or

```{r}
poly.lm = lm(y~poly(x,4), df) 
summary(poly.lm)
```

High degree polynomials will try to *interpolate* instead of getting a best fit. This leads to overfitting.

# Kernels

```{r}
library(kernlab)
```

Fit the nonlinear model using Gaussian process regression, auto selecting the hyperparameters.

```{r}
#kernel.model = gausspr(x,y) # defaults to var = 1e-3
kernel.model = gausspr(x,y, var=.2) # changes the value
```

`var` corresponds to $\gamma^2$ in

$$
\alpha = (K + \gamma^2 I)^{-1} y
$$

Defaults to using the RBF/Gaussian kernel.

```{r}
kernel.model
```

```{r}
x.test = seq(0,4,length.out = 200)
y.test = predict(kernel.model,x.test)
```

**NOTE**: By Default, this returns a 1 column matrix:

```{r}
y.test
```

Convert it to a vector:

```{r}
y.test = y.test |>as.vector()
```

```{r}
test.df = tibble(x.test, y.test)
```

```{r}
kernel.plt =ggplot(df, aes(x,y)) + geom_point()  +
  geom_line(data=test.df, aes(x=x.test, y=y.test), color="red",linewidth=2)
print(kernel.plt)
```
