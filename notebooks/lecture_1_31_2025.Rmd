---
title: "Lecture 1/31/2025"
output: html_document
date: "2025-01-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(ISLR2)
```

# Multilinear Regression

## Elementary Example

Learn

$$
Y = 5 + 2X_1 -3X_2 + \epsilon
$$

$p=2$ and we have noise in our measurements.

Create synthetic data:

```{r}
n = 25
beta0 = 5
beta1 = 2 
beta2 = -3

set.seed(123)
# create random points, these are arbitrary, we could use normals, or just seq
x1 = runif(n, min=-2,2)
x2 = runif(n, min=-2,2)

y = beta0 + beta1 * x1 + beta2 * x2 + rnorm(n, sd = 0.5)
```

Need to move this into a data frame

```{r}
example1.df = tibble(x1=x1, x2 =x2, y = y)
example1.df
```

Perform the regression:

```{r}
example1.lm = lm(y~x1 + x2,example1.df) # note implicitly includes the constant
```

NOTE: the only thing we needed to do to make this multilinear was to put in the formula `y~x1+x2` , with two variables.

```{r}
example1.lm
```

Check the quality of the fit:

```{r}
summary(example1.lm)
```

Note: The $R^2 \approx .99$, so this is a very good fit. All three coefficients are statistically significant.

```{r}
confint(example1.lm)
```

The 95% confidence intervals of the coefficients all include the truth.

## Example with Auto Dataset

```{r}
auto.df = as_tibble(Auto)
```

```{r}
auto.df
```

```{r}
auto.df$origin = as_factor(auto.df$origin)
auto.df$cylinders= as_factor(auto.df$cylinders)
```

Want to model `mpg` against the other features. `mpg` plays the role of the $Y$, the response variable.

The first step is to check pair correlations to see what actually matters:

```{r}
cor(select_if(auto.df, is.numeric)) # check pair correlations amongst numerical features
```

The three most strongly correlated predictors are `horsepower` , `displacement` , and `weight` .

Fit the model:

```{r}
auto.lm = lm(mpg~displacement+weight+horsepower, auto.df)
```

```{r}
auto.lm
```

```{r}
summary(auto.lm)
```

We get an $R^2\approx .7$. The $\hat{\beta}_1$ for `displacement` is not statistically significant:

```{r}
confint(auto.lm)
```

The others are statistically significant. We now iterate:

-   Drop `displacement` from the model

-   Try `year` instead, as it is the next most strongly correlated

```{r}
auto.lm2 = lm(mpg~weight+horsepower+year, auto.df)
summary(auto.lm2)
```

Remarks:

-   This is a better fit, as the $R^2$ is higher

-   This also suggests we do not need the `horsepower` . Previously, the horsepower was statistically significant.

Lets drop `horsepower` and try the last one:

```{r}
auto.lm3 = lm(mpg~weight+acceleration+year, auto.df)
summary(auto.lm3)
```

Note, we can extract just the $R^2$:

```{r}
auto3.summary = summary(auto.lm3)
```

```{r}
auto3.summary$r.squared
```

Final model:

```{r}
auto.lm4 = lm(mpg~weight+year, auto.df)
summary(auto.lm4)
```

This has the highest $R^2$, with only two predictors. Including more features is not necessarily better.

Check an alternative:

```{r}
auto.lm5 = lm(mpg~weight+acceleration, auto.df)
summary(auto.lm5)
```

Note the order in which we add features matters:

-   (`weight`, `acceleration`, `year` ) is much better than (`weight`, `acceleration`)

-   But (`weight`, `acceleration`, `year` ) is about the same as (`weight`, `year`)

### Collapse down the model

Start with all features, and sequentially remove them:

```{r}
names(auto.df)
```

```{r}
collapse.lm1 = lm(mpg~displacement+horsepower+weight+acceleration+year, auto.df)
summary(collapse.lm1)
```

We can definitely eliminate `horsepower` :

```{r}
collapse.lm2 = lm(mpg~displacement+weight+acceleration+year, auto.df)
summary(collapse.lm2)
```

We can definitely eliminate `displacement`:

```{r}
collapse.lm3 = lm(mpg~+weight+acceleration+year, auto.df)
summary(collapse.lm3)
```

We would next drop `acceleration`

You can drop more than one variable per iteration.

This kind of manual, sequential iteration is easy enough to do with \~10 predictors. We will need something else for \~100 predictors. This leads to LASSO.

# Nonlinear Fitting

## Motivation

```{r}
set.seed(100)
x = runif(100, min=0, max = 4)
y = x + .5 * x**2 -.25 * x**3 + rnorm(100, sd=.1)
df = tibble(x=x, y=y)
```

```{r}
xy.plt = ggplot(df, aes(x,y)) + geom_point()
print(xy.plt)
```

This is a plot of

$$
Y = X + .5 X^2 -.25 X^3 + \epsilon
$$

We will fit a **linear** model with **nonlinear** features.

## Try a series of polynomial models

#### Linear

This is bad, but this is an example of a bad fit:

```{r}
plt = ggplot(df, aes(x,y)) + geom_point() + 
  geom_smooth(method=lm, formula = y~x)
print(plt)
```

#### Quadratic

```{r}
quad.lm = lm(y~x+I(x^2), df) 
summary(quad.lm)
```

Note that the `I(x^2)` instructs it to compute the nonlinear transform of the predictor and regress with that.

This command solves the normal equations:

$$
\mathcal{X}^T\mathcal{X}\boldsymbol{\beta} = \mathcal{X}^T \boldsymbol{y}
$$

See how well this does, graphically: use `I` inside of `ggplot2`

```{r}
plt = ggplot(df, aes(x,y)) + geom_point() + 
  geom_smooth(method=lm, formula = y~x + I(x^2))
print(plt)
```

#### Cubic

```{r}
plt = ggplot(df, aes(x,y)) + geom_point() + 
  geom_smooth(method=lm, formula = y~x + I(x^2)+I(x^3))
print(plt)
```

```{r}
cubic.lm = lm(y~x+I(x^2)+I(x^3), df) 
summary(cubic.lm)
```

#### Quartic

```{r}
plt = ggplot(df, aes(x,y)) + geom_point() + 
  geom_smooth(method=lm, formula = y~x + I(x^2)+I(x^3)+I(x^4))
print(plt)
```

```{r}
quartic.lm = lm(y~x+I(x^2)+I(x^3)+I(x^4), df) 
summary(quartic.lm)
```

Notice the change in the $p$ values when we increase further, even though the fit is still quite good.

### General case

Rather than having to keep adding `I(x^k)` term, we can use `poly`, with the specified highest degree

```{r}
plt = ggplot(df, aes(x,y)) + geom_point() + 
  geom_smooth(method=lm, formula = y~poly(x,4))
print(plt)
```

or

```{r}
poly.lm = lm(y~poly(x,4), df) 
summary(poly.lm)
```
