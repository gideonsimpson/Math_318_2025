---
title: "Lecture 2/17/2025"
output: html_document
date: "2025-02-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ISLR2)
library(MASS) # for LDA and QDA regressions
```

# Bayesian Classifiers

## LDA in 2 Variables with More Classes

### Generate mixture model data

Create a Gaussian mixture model of three classes in 2D

```{r}
set.seed(100)

# true means
mu1=c(-1, -1)
mu2=c(-1, 1)
mu3=c(2,0)
# true covariances
sigma1vals=c(1., 1.)
sigma2vals=c(1., 1.)
sigma3vals=c(1., 2.)
# priors
prior=c(0.25, 0.5, 0.25)
# total number of samples
n = 10^4

# training data
# sample with replacement from 1,2,3 n times
# with prob(1) = pi[1], prob(2)=pi[2], prob(3)=pi[3]
class=sample(1:3, n, replace = TRUE, prob=prior)
# interpret as a categorical variable
class=factor(class)
x=numeric(n)
y=numeric(n)
for(j in 1:n){
  # loop through switching between cases
  if(class[j]==1){
    x[j] = rnorm(1,mean = mu1[1], sd = sigma1vals[1])
    y[j] = rnorm(1,mean = mu1[2], sd = sigma1vals[2])
  }else if(class[j]==2){
    x[j] = rnorm(1,mean = mu2[1], sd = sigma2vals[1])
    y[j] = rnorm(1,mean = mu2[2], sd = sigma2vals[2])
  }else{
    x[j] = rnorm(1,mean = mu3[1], sd = sigma3vals[1])
    y[j] = rnorm(1,mean = mu3[2], sd = sigma3vals[2])
  }
}
train.df <- tibble("class"=class, "x"=x, "y"=y)

```

```{r}
mixture2d.plt=ggplot(train.df, aes(color=class)) + 
  geom_point(aes(x=x,y=y))
print(mixture2d.plt)

```

```{r}
train.df
```

### Train the LDA

Similar calling as `lm` or `glm`

```{r}
lda.2d = lda(class~x+y, data = train.df)
print(lda.2d)
```

`Prior probabilites` are the estimates $\hat{\pi}_i$ and the `Group means` are the etimates of $\hat{m}_i$

### Check Performance

```{r}
pred.train=predict(lda.2d)
mean(pred.train$class != train.df$class) # mean classification error
table(pred.train$class, train.df$class) # confusion matrix

```

## QDA

### Train the QDA

```{r}
qda.2d = qda(class~x+y, data = train.df)
print(qda.2d)
```

### Check Performance

```{r}
pred.train=predict(qda.2d)
mean(pred.train$class != train.df$class)
table(pred.train$class, train.df$class)

```

A bit better than LDA

```{r}
plt.qda.df = tibble(x=train.df$x, y=train.df$y, 
                    predicted=as.factor(pred.train$class),
                    truth=train.df$class, 
                    correct =(pred.train$class==train.df$class) )

qda2d.plt = ggplot(plt.qda.df, 
                   aes(color=predicted, shape=correct,size=correct)) +
  geom_point(aes(x=x,y=y)) +
  scale_shape_manual(values = c(4,20)) +
  scale_size_manual(values=c(4,1))
print(qda2d.plt)
```

Note the decision boundaries are not necessarily linear, particularly against the blue set.

Alternative visualization:

```{r}
qda2d.err.plt = ggplot(plt.qda.df, 
                       aes(color=correct, shape=predicted)) +
  geom_point(aes(x=x,y=y)) 
print(qda2d.err.plt)
```

# Nearest Neighbors Classifiers

```{r}
library(FNN) # for kNN methods
```

Create a testing data set for this too:

```{r}
set.seed(500)

# true means
mu1 <- c(-1, -1)
mu2 <- c(-1, 1)
mu3 <- c(2,0)
# true variances
sigma1vals <- c(1., 1.)
sigma2vals <- c(1., 1.)
sigma3vals <- c(1., 2.)
# priors
prior <- c(0.25, 0.5, 0.25)
# total number of samples
n = 10^4

# training data
# sample with replacement from 1,2,3 n times
# with prob(1) = pi[1], prob(2)=pi[2], prob(3)=pi[3]
class<- sample(1:3, n, replace = TRUE, prob=prior)
# interpret as a categorical variable
class <- factor(class)
x <- numeric(n)
y <- numeric(n)
for(j in 1:n){
  # loop through switching between cases
  if(class[j]==1){
    x[j] <- rnorm(1,mean = mu1[1], sd = sigma1vals[1])
    y[j] <- rnorm(1,mean = mu1[2], sd = sigma1vals[2])
  }else if(class[j]==2){
    x[j] <- rnorm(1,mean = mu2[1], sd = sigma2vals[1])
    y[j] <- rnorm(1,mean = mu2[2], sd = sigma2vals[2])
  }else{
    x[j] <- rnorm(1,mean = mu3[1], sd = sigma3vals[1])
    y[j] <- rnorm(1,mean = mu3[2], sd = sigma3vals[2])
  }
}
test.df <- tibble("class"=class, "x"=x, "y"=y)
```

`knn` has two quirks to how it is called:

-   Must pass data values in as matrix type (`as.matrix`); vectors/data frames are not adequate

-   Call as `knn(training features, testing features, training classes`)

```{r}
knn.train.pred = knn(as.matrix(train.df[2:3]), 
                     as.matrix(train.df[2:3]), 
                     as.matrix(train.df[1]), k=3) # 3 nearest neighbors
mean(knn.train.pred != train.df$class)
table(knn.train.pred, train.df$class)
```

```{r}
knn.train.plt.df=tibble(x=train.df$x, y=train.df$y, predicted=as.factor(knn.train.pred), 
                 truth=train.df$class, correct =(knn.train.pred==train.df$class) )
knn.train.nn2d.plt = ggplot(knn.train.plt.df, 
                            aes(color=correct, shape=predicted)) + 
  geom_point(aes(x=x,y=y)) 
print(knn.train.nn2d.plt)
```

Testing data error:

```{r}
knn.test.pred = knn(as.matrix(train.df[2:3]), 
                     as.matrix(test.df[2:3]), 
                     as.matrix(train.df[1]), k=3) # 3 nearest neighbors
mean(knn.test.pred != test.df$class)
table(knn.test.pred, test.df$class)
```

Testing data has almost double the training error.

```{r}
knn.test.plt.df=tibble(x=test.df$x, y=test.df$y, predicted=as.factor(knn.test.pred), 
                 truth=test.df$class, correct =(knn.test.pred==test.df$class) )
knn.test.nn2d.plt = ggplot(knn.test.plt.df, 
                            aes(color=correct, shape=predicted)) + 
  geom_point(aes(x=x,y=y)) 
print(knn.test.nn2d.plt)
```
